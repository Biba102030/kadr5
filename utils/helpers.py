import logging
from io import BytesIO
from aiogram import types
from keyboards import get_back_to_main_menu
from config import MAX_MESSAGE_LENGTH, bot, MAX_ARTICLES
from parser import fetch_article_content, search_articles
from datetime import datetime
import aiohttp
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

async def send_article_content(chat_id: int, article: dict):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
    try:
        content = article.get("text") if "text" in article and article["text"] else await fetch_article_content(article["url"])
        if not content:
            await bot.send_message(chat_id, "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏", reply_markup=get_back_to_main_menu())
            return
        header = f"üì∞ {article['title']}\n"
        if article.get("date"):
            header += f"üìÖ {article['date']}\n"
        header += f"üîó {article['url']}\n\n"
        full_content = header + content
        if len(full_content) > MAX_MESSAGE_LENGTH:
            file = BytesIO(full_content.encode("utf-8"))
            file.name = f"{article['title'][:50]}.txt"
            await bot.send_document(chat_id, file, caption="üìÑ –°—Ç–∞—Ç—å—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ —Ñ–∞–π–ª–æ–º –∏–∑-–∑–∞ –±–æ–ª—å—à–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞", reply_markup=get_back_to_main_menu())
        else:
            await bot.send_message(chat_id, full_content, reply_markup=get_back_to_main_menu(), disable_web_page_preview=True)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Å—Ç–∞—Ç—å–∏: {e}")
        await bot.send_message(chat_id, "‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏", reply_markup=get_back_to_main_menu())

def format_search_results_text(articles, query):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    results_text = f"üîç **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞** –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}':\n\n"
    for i, article in enumerate(articles[:MAX_ARTICLES]):
        title = article["title"][:57] + "..." if len(article["title"]) > 60 else article["title"]
        results_text += f"{i+1}. **{title}**\n"
        if article.get("date"):
            results_text += f"   üìÖ {article['date']}\n"
        if article.get("text"):
            text_preview = article["text"][:100] + "..." if len(article["text"]) > 100 else article["text"]
            results_text += f"   üìù {text_preview}\n"
        results_text += "\n"
    results_text += f"üìä –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}\n–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ç–∞—Ç—å—é –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞:"
    return results_text

async def fetch_rubrika_articles(rubrika_slug):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –∏–∑ —Ä—É–±—Ä–∏–∫–∏ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫"""
    rubrika_queries = {
        "trudovoe-pravo": "—Ç—Ä—É–¥–æ–≤–æ–µ –ø—Ä–∞–≤–æ",
        "nalogi-vznosy": "–Ω–∞–ª–æ–≥–∏",
        "kadrovoe-deloproizvodstvo": "–∫–∞–¥—Ä–æ–≤–æ–µ –¥–µ–ª–æ–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ",
        "otpuska": "–æ—Ç–ø—É—Å–∫",
        "bolnichnye": "–±–æ–ª—å–Ω–∏—á–Ω—ã–π",
        "zarplata": "–∑–∞—Ä–ø–ª–∞—Ç–∞",
        "ohrana-truda": "–æ—Ö—Ä–∞–Ω–∞ —Ç—Ä—É–¥–∞",
        "prakticheskie-voprosy": "–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã"
    }
    
    query = rubrika_queries.get(rubrika_slug, rubrika_slug.replace("-", " "))
    print(f"{datetime.now()}: –ü–∞—Ä—Å–∏–Ω–≥ —Ä—É–±—Ä–∏–∫–∏ '{rubrika_slug}' —Å –ø–æ–∏—Å–∫–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–æ–º: {query}")
    
    try:
        articles = await search_articles(query, "ru")
        return articles[:MAX_ARTICLES]
    except Exception as e:
        print(f"{datetime.now()}: –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ä—É–±—Ä–∏–∫–∏ {rubrika_slug}: {e}")
        return []

async def fetch_topics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ç–µ–º –∏–∑ <ul class='tax-code__list'>"""
    base_url = "https://kadrovik.uz/"
    print(f"{datetime.now()}: –ü–∞—Ä—Å–∏–º —Ç–µ–º—ã —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {base_url}")
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(base_url, headers=headers, timeout=aiohttp.ClientTimeout(total=6)) as response:
                response.raise_for_status()
                text = await response.text()
                soup = BeautifulSoup(text, "html.parser")

        topics = []
        topic_list = soup.select("ul.tax-code__list li.tax-code__list-item a.tax-code__list-link")[:10]  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ 10 —Ç–µ–º
        if topic_list:
            print(f"{datetime.now()}: –ù–∞–π–¥–µ–Ω–æ {len(topic_list)} —Ç–µ–º –≤ <ul class='tax-code__list'>")
            for item in topic_list:
                url = item['href'] if item.get('href') else ''
                title = item.get_text(strip=True) if item.get_text(strip=True) else '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'
                
                if not url.startswith("http"):
                    url = base_url.rstrip("/") + "/" + url.lstrip("/")
                
                topics.append({
                    "title": title,
                    "url": url
                })
        
        print(f"{datetime.now()}: –ù–∞–π–¥–µ–Ω–æ —Ç–µ–º: {len(topics)}")
        return topics
    except Exception as e:
        print(f"{datetime.now()}: –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ç–µ–º: {e}")
        return []

async def fetch_topic_articles(topic_url):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–µ–º—ã"""
    print(f"{datetime.now()}: –ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç—å–∏ —Å —Ç–µ–º—ã: {topic_url}")
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(topic_url, headers=headers, timeout=aiohttp.ClientTimeout(total=6)) as response:
                response.raise_for_status()
                text = await response.text()
                soup = BeautifulSoup(text, "html.parser")

        articles = []
        posts_list = soup.select("ul.rec-selected__content-item li a.rec-block__info-post")
        if posts_list:
            print(f"{datetime.now()}: –ù–∞–π–¥–µ–Ω–æ {len(posts_list)} —Å—Ç–∞—Ç–µ–π –≤ —Ç–µ–º–µ")
            for item in posts_list[:MAX_ARTICLES]:
                title_tag = item.find('h3', class_='info-post__title-item')
                url_link = item['href'] if item.get('href') else ''
                title = title_tag.get_text(strip=True) if title_tag else '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'
                date = datetime.now().strftime('%d.%m.%Y')

                if not url_link.startswith("http"):
                    url_link = "https://kadrovik.uz/" + url_link.lstrip("/")

                articles.append({
                    "title": title,
                    "content": "",
                    "date": date,
                    "emoji": "üì∞",
                    "url": url_link
                })
        
        print(f"{datetime.now()}: –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –≤ —Ç–µ–º–µ: {len(articles)}")
        return articles[:MAX_ARTICLES]
    except Exception as e:
        print(f"{datetime.now()}: –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç–µ–π —Ç–µ–º—ã {topic_url}: {e}")
        return []