import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
from utils.storage import load_cache, save_cache

async def fetch_articles_from_site(query=None, lang="ru", limit=10):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π —Å —Å–∞–π—Ç–∞ Kadrovik.uz"""
    start_time = time.time()
    base_url = "https://kadrovik.uz/" if lang == "ru" else "https://kadrovik.uz/uz/"
    url = base_url if not query else f"{base_url}search?q={query}"
    print(f"{datetime.now()}: –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ URL: {url}")
    
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=6)) as response:
                response.raise_for_status()
                text = await response.text()
                soup = BeautifulSoup(text, "html.parser")

        articles = []
        
        if query:
            # –î–ª—è –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            results_list = soup.select("ol.results li")
            if results_list:
                print(f"{datetime.now()}: –ù–∞–π–¥–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–∏—Å–∫–∞ —Å–æ —Å—Ç–∞—Ä–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π")
                for li in results_list[:limit]:
                    a_tag = li.find('a')
                    span_date = li.find('span', class_='date')

                    url_link = a_tag['href'] if a_tag else ''
                    title = a_tag.get_text(strip=True) if a_tag else ''
                    date = span_date.get_text(strip=True) if span_date else ''
                          
                    if a_tag:
                        a_tag.extract()
                    if span_date:
                        span_date.extract()

                    text_content = li.get_text(separator=' ', strip=True)

                    if not url_link.startswith("http"):
                        url_link = base_url.rstrip("/") + "/" + url_link.lstrip("/")

                    articles.append({
                        "title": title or "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞",
                        "content": text_content or "",
                        "date": date or datetime.now().isoformat(),
                        "emoji": "üì∞",
                        "url": url_link
                    })
        else:
            # –î–ª—è –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä—Å–∏–º <ul class="posts-list">
            print(f"{datetime.now()}: –ü–∞—Ä—Å–∏–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É, –∏—â–µ–º <ul class='posts-list'>")
            posts_list = soup.select("ul.posts-list li.post-card-wrapper, ul.posts-list li.post-card--horizontal-wrapper")
            if posts_list:
                print(f"{datetime.now()}: –ù–∞–π–¥–µ–Ω–æ {len(posts_list)} —Å—Ç–∞—Ç–µ–π –≤ <ul class='posts-list'>")
                for item in posts_list[:limit]:
                    a_tag = item.find('a', href=True)
                    title_tag = item.find('h4', class_='post-card__title')
                    date_tag = item.find('time', class_='longread-post__time-published')

                    url_link = a_tag['href'] if a_tag else ''
                    title = title_tag.get_text(strip=True) if title_tag else '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'
                    date = date_tag.get_text(strip=True) if date_tag else datetime.now().strftime('%d.%m.%Y')

                    if not url_link.startswith("http"):
                        url_link = base_url.rstrip("/") + "/" + url_link.lstrip("/")

                    articles.append({
                        "title": title,
                        "content": "",  # –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –∏–∑–≤–ª–µ–∫–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ –µ–≥–æ –Ω–µ—Ç –≤ HTML
                        "date": date,
                        "emoji": "üì∞",
                        "url": url_link
                    })
            
            # –†–µ–∑–µ—Ä–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞
            if not articles:
                print(f"{datetime.now()}: –ò—â–µ–º —Å—Ç–∞—Ç—å–∏ –≤ –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Å–∞–π—Ç–∞")
                article_links = []
                potential_selectors = [
                    "a[href*='/publish/']",
                    "a[href*='/article/']",
                    ".article-link",
                    ".publication-link",
                    "article a",
                    ".content a[href]",
                    "main a[href]"
                ]
                
                for selector in potential_selectors:
                    links = soup.select(selector)
                    if links:
                        print(f"{datetime.now()}: –ù–∞–π–¥–µ–Ω—ã —Å—Å—ã–ª–∫–∏ —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º: {selector} ({len(links)} —à—Ç—É–∫)")
                        article_links.extend(links)
                        break
                
                if not article_links:
                    all_links = soup.find_all('a', href=True)
                    article_links = [
                        link for link in all_links 
                        if any(keyword in link['href'] for keyword in ['/publish/', '/article/', '/news/'])
                        and not any(skip in link['href'] for skip in ['search', 'group', 'recent_publications'])
                    ]
                    print(f"{datetime.now()}: –ù–∞–π–¥–µ–Ω—ã –æ–±—â–∏–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏: {len(article_links)}")
                
                processed_urls = set()
                for link in article_links[:limit*2]:
                    if len(articles) >= limit:
                        break
                        
                    href = link.get('href', '')
                    if not href or href in processed_urls:
                        continue
                        
                    if not href.startswith("http"):
                        href = base_url.rstrip("/") + "/" + href.lstrip("/")
                    
                    processed_urls.add(href)
                    
                    title = link.get_text(strip=True)
                    if not title:
                        continue
                    
                    date = ""
                    parent = link.parent
                    if parent:
                        date_elem = parent.find('time') or parent.find(class_='date') or parent.find('span', string=lambda text: text and any(month in text for month in ['—è–Ω–≤–∞—Ä—è', '—Ñ–µ–≤—Ä–∞–ª—è', '–º–∞—Ä—Ç–∞', '–∞–ø—Ä–µ–ª—è', '–º–∞—è', '–∏—é–Ω—è']))
                        if date_elem:
                            date = date_elem.get_text(strip=True)
                    
                    content = ""
                    if parent:
                        text_nodes = parent.find_all(text=True)
                        content_parts = []
                        for text_node in text_nodes:
                            text = text_node.strip()
                            if text and text != title and len(text) > 10:
                                content_parts.append(text)
                        content = ' '.join(content_parts[:2])
                    
                    articles.append({
                        "title": title,
                        "content": content[:200] + "..." if len(content) > 200 else content,
                        "date": date or datetime.now().isoformat(),
                        "emoji": "üì∞",
                        "url": href
                    })

        print(f"{datetime.now()}: –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –ø–µ—Ä–µ–¥ —Å—Ä–µ–∑–æ–º: {len(articles)}")
        articles = articles[:limit]
        print(f"{datetime.now()}: –í–æ–∑–≤—Ä–∞—â–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –ø–æ—Å–ª–µ —Å—Ä–µ–∑–∞: {len(articles)}")
        
        if not articles:
            print(f"{datetime.now()}: –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏ –ø–æ URL: {url}")
            all_links = soup.find_all('a', href=True)[:5]
            for i, link in enumerate(all_links):
                print(f"{datetime.now()}: {i+1}. {link.get('href')} - {link.get_text(strip=True)[:50]}")
        
        print(f"{datetime.now()}: –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Ä–µ–º—è: {time.time() - start_time:.2f} —Å–µ–∫. –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
        
        cache = load_cache()
        cache_key = f"latest_{lang}" if not query else f"search_{query}_{lang}"
        cache[cache_key] = {
            "timestamp": datetime.now().isoformat(),
            "data": articles
        }
        save_cache(cache)
        return articles
    except Exception as e:
        print(f"{datetime.now()}: –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å–∞–π—Ç–∞: {e}. –í—Ä–µ–º—è: {time.time() - start_time:.2f} —Å–µ–∫")
        cache = load_cache()
        cache_key = f"latest_{lang}" if not query else f"search_{query}_{lang}"
        return cache.get(cache_key, {}).get("data", [])

async def fetch_article_content(url):
    """–ü–∞—Ä—Å–µ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ emoji –∏ –∞–±–∑–∞—Ü–µ–≤"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=10) as response:
                response.raise_for_status()
                html = await response.text()
        
        soup = BeautifulSoup(html, 'html.parser')
        
        title = soup.find('h1').get_text(strip=True) if soup.find('h1') else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
        date_elem = soup.find('time', {'class': 'longread-post__time-published'})
        date = date_elem['datetime'] if date_elem else ""
        
        content_block = soup.find('section', {'class': 'longread-block'}) or soup.find('body')
        
        if not content_block:
            return f"üì∞ {title}\nüìÖ {date}\n\n–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç."
        
        elements = content_block.find_all(['p', 'strong'])
        result = []
        
        for element in elements:
            text = element.get_text(' ', strip=True)
            if text:
                if element.name == 'strong':
                    result.append(f"\n \nüîπ {text}\n")
                else:
                    result.append(f"{text}")
        
        content = ''.join(dict.fromkeys(result))
        
        return content if len(content) > 50 else "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç."
    
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")
        return None

async def search_articles(query, lang):
    """–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
    cache = load_cache()
    cache_key = f"search_{query}_{lang}"
    
    if cache_key in cache:
        entry = cache[cache_key]
        timestamp = entry.get("timestamp")
        if timestamp and (datetime.now() - datetime.fromisoformat(timestamp)) < timedelta(hours=24):
            print(f"{datetime.now()}: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
            return entry["data"]

    print(f"{datetime.now()}: –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query}")
    articles = await fetch_articles_from_site(query, lang)
    return articles

async def get_latest_articles(lang):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π"""
    cache = load_cache()
    cache_key = f"latest_{lang}"
    
    if cache_key in cache:
        entry = cache[cache_key]
        timestamp = entry.get("timestamp")
        if timestamp and (datetime.now() - datetime.fromisoformat(timestamp)) < timedelta(hours=24):
            print(f"{datetime.now()}: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π ({lang})")
            return entry["data"]

    print(f"{datetime.now()}: –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π ({lang})")
    articles = await fetch_articles_from_site(lang=lang, limit=5)  # –õ–∏–º–∏—Ç 5 –¥–ª—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    return articles